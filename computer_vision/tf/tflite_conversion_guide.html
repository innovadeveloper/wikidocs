<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TensorFlow Lite Conversion Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Arial, sans-serif;
            line-height: 1.5;
            color: #000;
            background: #fff;
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        
        h1 {
            font-size: 32px;
            font-weight: 700;
            margin-bottom: 40px;
            text-align: center;
        }
        
        h2 {
            font-size: 20px;
            font-weight: 600;
            margin: 30px 0 15px 0;
            border-bottom: 1px solid #000;
            padding-bottom: 5px;
        }
        
        h3 {
            font-size: 16px;
            font-weight: 600;
            margin: 20px 0 10px 0;
        }
        
        .conversion-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin: 40px 0;
        }
        
        .method-card {
            border: 1px solid #000;
            padding: 20px;
        }
        
        .method-title {
            font-weight: 600;
            font-size: 18px;
            margin-bottom: 15px;
        }
        
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 15px 0;
        }
        
        .pros, .cons {
            font-size: 14px;
        }
        
        .pros h4, .cons h4 {
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .pros ul, .cons ul {
            list-style: none;
            padding-left: 0;
        }
        
        .pros li::before {
            content: "+ ";
            font-weight: bold;
        }
        
        .cons li::before {
            content: "- ";
            font-weight: bold;
        }
        
        .code-block {
            background: #f5f5f5;
            border: 1px solid #ddd;
            padding: 15px;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 12px;
            margin: 10px 0;
            white-space: pre-wrap;
        }
        
        .optimization-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 14px;
        }
        
        .optimization-table th,
        .optimization-table td {
            border: 1px solid #000;
            padding: 10px;
            text-align: left;
        }
        
        .optimization-table th {
            background: #f5f5f5;
            font-weight: 600;
        }
        
        .decision-flow {
            display: flex;
            flex-direction: column;
            gap: 20px;
            margin: 30px 0;
        }
        
        .decision-box {
            border: 1px solid #000;
            padding: 15px;
            text-align: center;
        }
        
        .decision-box.start {
            background: #f0f0f0;
        }
        
        .arrow {
            text-align: center;
            font-size: 18px;
            font-weight: bold;
        }
        
        .warning-box {
            border: 2px solid #000;
            padding: 15px;
            margin: 20px 0;
            background: #f9f9f9;
        }
        
        .warning-title {
            font-weight: 600;
            margin-bottom: 10px;
        }
        
        @media (max-width: 768px) {
            .conversion-grid {
                grid-template-columns: 1fr;
            }
            
            .pros-cons {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <h1>TensorFlow Lite Model Conversion Guide</h1>
    
    <div class="conversion-grid">
        <div class="method-card">
            <div class="method-title">Direct Export (Framework Native)</div>
            <div class="code-block">model = YOLO("yolov8n.pt")
model.export(format="tflite")</div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h4>Advantages</h4>
                    <ul>
                        <li>One line of code</li>
                        <li>Framework handles optimization</li>
                        <li>Tested and reliable</li>
                        <li>Automatic error handling</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>Limitations</h4>
                    <ul>
                        <li>Limited to specific frameworks</li>
                        <li>Less control over optimization</li>
                        <li>May not support all models</li>
                    </ul>
                </div>
            </div>
        </div>
        
        <div class="method-card">
            <div class="method-title">Manual Conversion (TFLiteConverter)</div>
            <div class="code-block">converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()</div>
            
            <div class="pros-cons">
                <div class="pros">
                    <h4>Advantages</h4>
                    <ul>
                        <li>Full control over optimization</li>
                        <li>Works with any TensorFlow model</li>
                        <li>Custom quantization options</li>
                        <li>Fine-tuned for specific hardware</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>Limitations</h4>
                    <ul>
                        <li>Requires deep understanding</li>
                        <li>More code and complexity</li>
                        <li>Manual error handling</li>
                        <li>Time-consuming setup</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    
    <h2>Conversion Methods by Source</h2>
    
    <table class="optimization-table">
        <thead>
            <tr>
                <th>Source Format</th>
                <th>Conversion Method</th>
                <th>Complexity</th>
                <th>Success Rate</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Keras Model (.h5)</td>
                <td>TFLiteConverter.from_keras_model()</td>
                <td>Low</td>
                <td>High</td>
            </tr>
            <tr>
                <td>SavedModel</td>
                <td>TFLiteConverter.from_saved_model()</td>
                <td>Low</td>
                <td>High</td>
            </tr>
            <tr>
                <td>YOLO (Ultralytics)</td>
                <td>model.export(format="tflite")</td>
                <td>Very Low</td>
                <td>High</td>
            </tr>
            <tr>
                <td>PyTorch Model</td>
                <td>PyTorch → ONNX → TF → TFLite</td>
                <td>High</td>
                <td>Medium</td>
            </tr>
            <tr>
                <td>TensorFlow Hub</td>
                <td>from_concrete_functions()</td>
                <td>Medium</td>
                <td>Medium</td>
            </tr>
        </tbody>
    </table>
    
    <h2>Optimization Strategies</h2>
    
    <div class="conversion-grid">
        <div class="method-card">
            <h3>Dynamic Range Quantization</h3>
            <div class="code-block">converter.optimizations = [tf.lite.Optimize.DEFAULT]</div>
            <p><strong>Reduces model size by 50%</strong><br>
            Converts weights to 8-bit, keeps operations in float32. Best balance of size and accuracy.</p>
        </div>
        
        <div class="method-card">
            <h3>Full Integer Quantization</h3>
            <div class="code-block">converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]</div>
            <p><strong>Reduces model size by 75%</strong><br>
            Converts everything to 8-bit. Requires representative dataset. Best for edge devices.</p>
        </div>
    </div>
    
    <h2>Decision Flow</h2>
    
    <div class="decision-flow">
        <div class="decision-box start">
            <strong>Start: Need TFLite Model</strong>
        </div>
        
        <div class="arrow">↓</div>
        
        <div class="decision-box">
            Is your framework Ultralytics YOLO, MediaPipe, or similar?
        </div>
        
        <div class="arrow">Yes → Use Direct Export | No ↓</div>
        
        <div class="decision-box">
            Is your model in TensorFlow/Keras format?
        </div>
        
        <div class="arrow">Yes → Use TFLiteConverter | No ↓</div>
        
        <div class="decision-box">
            Convert to TensorFlow first, then use TFLiteConverter
        </div>
    </div>
    
    <div class="warning-box">
        <div class="warning-title">Critical Considerations</div>
        <ul>
            <li><strong>Model Size vs Accuracy:</strong> More optimization = smaller size but potential accuracy loss</li>
            <li><strong>Hardware Target:</strong> Choose quantization based on deployment device capabilities</li>
            <li><strong>Representative Data:</strong> For INT8 quantization, use real data samples from your use case</li>
            <li><strong>Operator Support:</strong> Some TensorFlow operations may not be supported in TFLite</li>
            <li><strong>Testing Required:</strong> Always validate converted model accuracy before deployment</li>
        </ul>
    </div>
    
    <h2>Common Issues and Solutions</h2>
    
    <div class="conversion-grid">
        <div class="method-card">
            <h3>Unsupported Operators</h3>
            <div class="code-block">converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]</div>
            <p>Include additional TensorFlow operations when TFLite builtins are insufficient.</p>
        </div>
        
        <div class="method-card">
            <h3>Dynamic Shapes</h3>
            <div class="code-block">converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32</div>
            <p>Fix input/output types to resolve shape inference issues during conversion.</p>
        </div>
        
        <div class="method-card">
            <h3>Large Model Size</h3>
            <div class="code-block">converter.target_spec.supported_types = [tf.float16]</div>
            <p>Use float16 quantization for significant size reduction with minimal accuracy impact.</p>
        </div>
        
        <div class="method-card">
            <h3>Conversion Failures</h3>
            <div class="code-block">converter.allow_custom_ops = True
converter.experimental_new_converter = True</div>
            <p>Enable experimental features and custom operations for complex models.</p>
        </div>
    </div>
    
    <div class="warning-box">
        <div class="warning-title">Production Deployment Checklist</div>
        <ul>
            <li>Validate model accuracy against original</li>
            <li>Benchmark inference speed on target hardware</li>
            <li>Test with representative input data</li>
            <li>Verify memory usage within device constraints</li>
            <li>Document conversion parameters for reproducibility</li>
        </ul>
    </div>
</body>
</html>